{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from dask.distributed import Client\n",
    "\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "from prob_unet_utils import plot_losses, plot_losses_mae\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Probabilistic Unet Latent dim: 16\n",
      "Epoch 1/30 - beta_0: 1.0, beta_1: 0.0000, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/30:   0%|                                                                                                | 0/172 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [01:01<00:00,  2.81it/s, Loss: 0.3244]\n",
      ":: Evaluation :::   4%|████                                                                                         | 2/46 [00:00<00:04, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.03it/s, Loss: 0.2489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - beta_0: 1.0, beta_1: 0.0000, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 2/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.87it/s, Loss: 0.2296]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.45it/s, Loss: 0.2126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - beta_0: 1.0, beta_1: 0.0025, beta_2: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 3/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.3282]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.42it/s, Loss: 0.2716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - beta_0: 1.0, beta_1: 0.2129, beta_2: 0.2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 4/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [00:59<00:00,  2.87it/s, Loss: 0.2806]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.18it/s, Loss: 0.2521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - beta_0: 1.0, beta_1: 147.5023, beta_2: 28.5499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 5/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.87it/s, Loss: 0.2535]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.38it/s, Loss: 0.2394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - beta_0: 1.0, beta_1: 30457.3899, beta_2: 19341.3472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 6/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.3347]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.38it/s, Loss: 0.2265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - beta_0: 1.0, beta_1: 401854.3078, beta_2: 604939.7277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 7/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.2687]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.41it/s, Loss: 0.2199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - beta_0: 1.0, beta_1: 5336801.0101, beta_2: 8862218.6966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 8/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 1.8287]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.43it/s, Loss: 0.2128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - beta_0: 1.0, beta_1: 2881563.4943, beta_2: 7526583.7483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 9/30: 100%|████████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.2122]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.40it/s, Loss: 0.2075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - beta_0: 1.0, beta_1: 9998016.7316, beta_2: 9998494.4737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 10/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.2066]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.34it/s, Loss: 0.2018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - beta_0: 1.0, beta_1: 9997831.8358, beta_2: 9998773.3681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 11/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.85it/s, Loss: 0.2014]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.43it/s, Loss: 0.1969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - beta_0: 1.0, beta_1: 9998371.5979, beta_2: 9999044.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 12/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1966]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.43it/s, Loss: 0.1941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - beta_0: 1.0, beta_1: 9998123.2746, beta_2: 9999161.8363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 13/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1931]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.37it/s, Loss: 0.1913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - beta_0: 1.0, beta_1: 9996373.8088, beta_2: 9999066.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 14/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.87it/s, Loss: 0.1905]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.44it/s, Loss: 0.1914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - beta_0: 1.0, beta_1: 9989079.6203, beta_2: 9998584.0449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 15/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1895]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.37it/s, Loss: 0.1901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - beta_0: 1.0, beta_1: 9973667.8736, beta_2: 9997123.6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 16/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1871]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.37it/s, Loss: 0.1871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - beta_0: 1.0, beta_1: 9971660.2375, beta_2: 9996602.9810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 17/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1892]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.38it/s, Loss: 0.2041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - beta_0: 1.0, beta_1: 9934019.3217, beta_2: 9992036.8025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 18/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [00:59<00:00,  2.87it/s, Loss: 0.1914]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.31it/s, Loss: 0.1845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - beta_0: 1.0, beta_1: 9897929.2554, beta_2: 9988063.7073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 19/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [00:59<00:00,  2.87it/s, Loss: 0.1940]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.42it/s, Loss: 0.1993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - beta_0: 1.0, beta_1: 9858520.7264, beta_2: 9983531.7282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 20/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.87it/s, Loss: 0.1797]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.45it/s, Loss: 0.1900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - beta_0: 1.0, beta_1: 9971838.6869, beta_2: 9996477.2825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 21/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [00:59<00:00,  2.87it/s, Loss: 0.1898]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.40it/s, Loss: 0.1874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - beta_0: 1.0, beta_1: 9865790.4513, beta_2: 9986254.1330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 22/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1892]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.39it/s, Loss: 0.1891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - beta_0: 1.0, beta_1: 9857925.8388, beta_2: 9984174.4374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 23/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1813]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.41it/s, Loss: 0.2081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - beta_0: 1.0, beta_1: 9916328.2650, beta_2: 9991102.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 24/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.87it/s, Loss: 0.1865]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.43it/s, Loss: 0.1906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - beta_0: 1.0, beta_1: 9857256.9660, beta_2: 9984337.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 25/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1808]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.42it/s, Loss: 0.1824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - beta_0: 1.0, beta_1: 9897491.8556, beta_2: 9986887.2214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 26/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [00:59<00:00,  2.87it/s, Loss: 0.1882]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.36it/s, Loss: 0.1858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - beta_0: 1.0, beta_1: 9821869.1908, beta_2: 9979343.4846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 27/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [00:59<00:00,  2.87it/s, Loss: 0.1766]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.43it/s, Loss: 0.1855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - beta_0: 1.0, beta_1: 9910948.2869, beta_2: 9989127.3743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 28/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.87it/s, Loss: 0.1820]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.45it/s, Loss: 0.1916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - beta_0: 1.0, beta_1: 9856900.6449, beta_2: 9982682.3217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 29/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.87it/s, Loss: 0.1830]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.35it/s, Loss: 0.1882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - beta_0: 1.0, beta_1: 9835760.7452, beta_2: 9980321.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 30/30: 100%|███████████████████████████████████████████████████████████████████████| 172/172 [01:00<00:00,  2.86it/s, Loss: 0.1732]\n",
      ":: Evaluation ::: 100%|██████████████████████████████████████████████████████████████████████████████| 46/46 [00:05<00:00,  8.37it/s, Loss: 0.1875]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed) \n",
    "        np.random.seed(seed)  \n",
    "        torch.manual_seed(seed) \n",
    "        torch.cuda.manual_seed(seed)  \n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "        torch.backends.cudnn.deterministic = True  \n",
    "        torch.backends.cudnn.benchmark = False  \n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # Set seed for reproducibility   \n",
    "    set_seed(42)  \n",
    "\n",
    "    # Importing all required arguments\n",
    "    args = tm.get_args()\n",
    "    args.lowres_scale = 16\n",
    "    args.num_epochs = 30\n",
    "    args.batch_size = 64\n",
    "\n",
    "    # Initializing the Probabilistic UNet model\n",
    "    probunet_model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=16,\n",
    "        num_filters=[32, 64, 128, 256],\n",
    "        model_channels=32,\n",
    "        channel_mult= [1, 2, 4, 8],\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "\n",
    "    # Initializing the datasets\n",
    "    dataset_train = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_train,\n",
    "        variables=args.variables,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale\n",
    "    )\n",
    "    \n",
    "    dataset_val = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_val,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "    # Initializing the dataloaders\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test_random = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # Initializing training objects\n",
    "    optimizer = args.optimizer(params=probunet_model.parameters(), lr=args.lr)\n",
    "    # optimizer = torch.optim.Adam(probunet_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "    # Initialize loss tracking lists for each variable\n",
    "    tr_losses_mae = {var: [] for var in args.variables}\n",
    "    tr_losses_kl = {var: [] for var in args.variables}\n",
    "    tr_losses_kl2 = {var: [] for var in args.variables}\n",
    "    val_losses_mae = {var: [] for var in args.variables}\n",
    "    val_losses_kl = {var: [] for var in args.variables}\n",
    "    val_losses_kl2 = {var: [] for var in args.variables}\n",
    "\n",
    "    beta_0 = 1.0\n",
    "    beta_1 = 0.00\n",
    "    beta_2 = 0.00\n",
    "        \n",
    "    warmup_epochs = 1\n",
    "    # Training loop\n",
    "    print(f\"Probabilistic Unet Latent dim: {probunet_model.latent_dim}\")\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "\n",
    "        probunet_model.beta_0 = beta_0\n",
    "        probunet_model.beta_1 = beta_1\n",
    "        probunet_model.beta_2 = beta_2\n",
    "\n",
    "    \n",
    "        print(f\"Epoch {epoch}/{args.num_epochs} - beta_0: {probunet_model.beta_0}, beta_1: {probunet_model.beta_1:.4f}, beta_2: {probunet_model.beta_2:.4f}\")\n",
    "\n",
    "        # Training for one epoch\n",
    "        train_losses_mae, training_losses_kl, training_losses_kl2, kl_div, kl_div2 = tm.train_probunet_step(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_train,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            num_epochs=args.num_epochs,\n",
    "            device=args.device,\n",
    "            variables=args.variables,\n",
    "        )\n",
    "        for var in args.variables:\n",
    "            tr_losses_mae[var].append(train_losses_mae[var])\n",
    "            tr_losses_kl[var].append(training_losses_kl[var])\n",
    "            tr_losses_kl2[var].append(training_losses_kl2[var])\n",
    "        \n",
    "        # Compute average losses for each term\n",
    "        avg_recon_loss = sum(train_losses_mae.values()) / len(train_losses_mae)  # Average reconstruction loss\n",
    "        avg_kl_loss = sum(training_losses_kl.values()) / len(training_losses_kl)      # Average KL (posterior vs prior)\n",
    "        avg_kl2_loss = sum(training_losses_kl2.values()) / len(training_losses_kl2)  # Average KL (posterior vs Gaussian)\n",
    "\n",
    "        # Ensure losses are scalars by detaching and converting them\n",
    "        # avg_recon_loss = float(avg_recon_loss.detach().cpu().item())  # Detach and convert to scalar\n",
    "        avg_kl_loss = float(avg_kl_loss.detach().cpu().item())\n",
    "        avg_kl2_loss = float(avg_kl2_loss.detach().cpu().item())\n",
    "        \n",
    "\n",
    "        if epoch > warmup_epochs:\n",
    "            # beta_0 = 1.0 / (avg_recon_loss + 1e-7)  \n",
    "            beta_0 = 1.0 \n",
    "            beta_1 = 1.0 / (avg_kl_loss + 1e-7)\n",
    "            beta_2 = 1.0 / (avg_kl2_loss + 1e-7)\n",
    "\n",
    "        else:\n",
    "            beta_0 = 1.0\n",
    "            beta_1 = 0.00\n",
    "            beta_2 = 0.00\n",
    "        \n",
    "        \n",
    "        # Evaluating the model on validation data\n",
    "        val_losses_mae_running, val_losses_kl_running, val_losses_kl2_running = tm.eval_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_val,\n",
    "            reconstruct=False,\n",
    "            device=args.device,           \n",
    "        )\n",
    "        for var in args.variables:\n",
    "            val_losses_mae[var].append(val_losses_mae_running[var])\n",
    "            val_losses_kl[var].append(val_losses_kl_running[var])\n",
    "            val_losses_kl2[var].append(val_losses_kl2_running[var])\n",
    "    \n",
    "        \n",
    "        test_batch = next(iter(dataloader_test_random))\n",
    "\n",
    "        residual_preds, (fig, axs) = tm.sample_residual_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_residuals.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig_difs, axs_difs = dataset_test.plot_residual_differences(\n",
    "        residual_preds=residual_preds,\n",
    "        timestamps_float=test_batch['timestamps_float'][:2],\n",
    "        epoch=epoch,\n",
    "        N=2, \n",
    "        num_samples=3\n",
    "        )\n",
    "        fig_difs.savefig(f\"{args.plotdir}/epoch{epoch}_res_difs.png\", dpi=300)\n",
    "        plt.close(fig_difs)\n",
    "\n",
    "        samples, (fig, axs) = tm.sample_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_reconstructed.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Save losses to a file after training\n",
    "    losses_to_save = {\n",
    "        \"train_losses_mae\": tr_losses_mae,\n",
    "        \"train_losses_kl\": tr_losses_kl,\n",
    "        \"train_losses_kl2\": tr_losses_kl2,\n",
    "        \"val_losses_mae\": val_losses_mae,\n",
    "        \"val_losses_kl\": val_losses_kl,\n",
    "        \"val_losses_kl2\": val_losses_kl2\n",
    "    }\n",
    "    with open(f\"{args.plotdir}/losses.pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses_to_save, f)\n",
    "\n",
    "    torch.save(probunet_model.state_dict(), f\"{args.plotdir}/probunet_model_lat_dim_{probunet_model.latent_dim}.pth\")\n",
    "\n",
    "    # # Plot training and validation loss curves for each variable\n",
    "    # plot_losses(tr_losses_mae, tr_losses_kl, val_losses_mae, val_losses_kl, args.variables, args.plotdir)\n",
    "\n",
    "    plot_losses(tr_losses_mae, tr_losses_kl, tr_losses_kl2, val_losses_mae, val_losses_kl, val_losses_kl2, args.variables, args.plotdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
