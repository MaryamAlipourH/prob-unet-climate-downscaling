{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "from prob_unet_utils import plot_losses, plot_losses_mae\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Probabilistic Unet Latent dim: 16\n",
      "Epoch 1/10 - beta_0: 1.0000, beta_1: 0.0000, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/10:   0%|                                                                                                                        | 0/343 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:44<00:00,  3.28it/s, Loss: 0.1232]\n",
      ":: Evaluation :::   2%|██▌                                                                                                                  | 2/92 [00:00<00:08, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1687, KL=1406.9236, KL2=114.1327 | [Val] CRPS=0.1134, KL=1476.5981, KL2=137.1654\n",
      "Epoch 2/10 - beta_0: 1.0000, beta_1: 0.0000, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:46<00:00,  3.23it/s, Loss: 0.1168]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.0938, KL=1420.8607, KL2=134.8637 | [Val] CRPS=0.0980, KL=1178.9982, KL2=133.1848\n",
      "Epoch 3/10 - beta_0: 1.0000, beta_1: 0.0000, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:43<00:00,  3.30it/s, Loss: 0.0986]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.0853, KL=1236.5480, KL2=141.5430 | [Val] CRPS=0.0879, KL=1139.2676, KL2=148.9180\n",
      "Epoch 4/10 - beta_0: 1.0000, beta_1: 0.0008, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 4/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:46<00:00,  3.23it/s, Loss: 0.1102]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.0897, KL=16.4645, KL2=47.0046 | [Val] CRPS=0.0894, KL=7.2868, KL2=45.7287\n",
      "Epoch 5/10 - beta_0: 1.0000, beta_1: 0.0607, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 5/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:43<00:00,  3.30it/s, Loss: 0.1060]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1377, KL=27.9318, KL2=15.8227 | [Val] CRPS=0.0962, KL=0.1292, KL2=16.4920\n",
      "Epoch 6/10 - beta_0: 1.0000, beta_1: 0.0358, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 6/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:46<00:00,  3.23it/s, Loss: 0.0918]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.0840, KL=0.1055, KL2=20.8320 | [Val] CRPS=0.0885, KL=0.0770, KL2=22.2767\n",
      "Epoch 7/10 - beta_0: 1.0000, beta_1: 9.4782, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 7/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:43<00:00,  3.31it/s, Loss: 0.1288]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1342, KL=0.0861, KL2=0.2853 | [Val] CRPS=0.1222, KL=0.0001, KL2=0.0662\n",
      "Epoch 8/10 - beta_0: 1.0000, beta_1: 11.6115, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 8/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:46<00:00,  3.23it/s, Loss: 0.0949]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1025, KL=0.0001, KL2=0.0689 | [Val] CRPS=0.1059, KL=0.0001, KL2=0.0733\n",
      "Epoch 9/10 - beta_0: 1.0000, beta_1: 11467.1021, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 9/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:43<00:00,  3.30it/s, Loss: 0.1333]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.0941, KL=0.0003, KL2=0.0185 | [Val] CRPS=0.1011, KL=0.0000, KL2=0.0160\n",
      "Epoch 10/10 - beta_0: 1.0000, beta_1: 3952.0212, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 10/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 343/343 [01:45<00:00,  3.24it/s, Loss: 0.1084]\n",
      ":: Evaluation ::: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:09<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.0880, KL=0.0000, KL2=0.0160 | [Val] CRPS=0.0966, KL=0.0000, KL2=0.0159\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed) \n",
    "        np.random.seed(seed)  \n",
    "        torch.manual_seed(seed) \n",
    "        torch.cuda.manual_seed(seed)  \n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "        torch.backends.cudnn.deterministic = True  \n",
    "        torch.backends.cudnn.benchmark = False  \n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # -- 1) Set seed for reproducibility\n",
    "    set_seed(42)  \n",
    "\n",
    "    # -- 2) Importing all required arguments\n",
    "    args = tm.get_args()\n",
    "    args.lowres_scale = 16\n",
    "    args.batch_size = 32\n",
    "    args.num_epochs = 10\n",
    "\n",
    "    # Initialize the Probabilistic UNet model\n",
    "    probunet_model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=16,\n",
    "        num_filters=[32, 64, 128, 256],\n",
    "        model_channels=32,\n",
    "        channel_mult=[1, 2, 4, 8],\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "\n",
    "    # -- 3) Prepare datasets\n",
    "    dataset_train = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_train,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_val = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_val,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "    # -- 4) Build DataLoaders\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test_random = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # -- 5) Define optimizer\n",
    "    optimizer = args.optimizer(params=probunet_model.parameters(), lr=args.lr)\n",
    "    # Example alternative:\n",
    "    # optimizer = torch.optim.Adam(probunet_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "    # -- 6) We track CRPS, KL, KL2 each epoch for train/val\n",
    "    train_crps_list, train_kl_list, train_kl2_list = [], [], []\n",
    "    val_crps_list,   val_kl_list,   val_kl2_list   = [], [], []\n",
    "\n",
    "    # For convenience, we keep your adaptive betas:\n",
    "    beta_0 = 1.0\n",
    "    beta_1 = 0.0\n",
    "    beta_2 = 0.0\n",
    "    warmup_epochs = 2\n",
    "\n",
    "    print(f\"Probabilistic Unet Latent dim: {probunet_model.latent_dim}\")\n",
    "\n",
    "    # -- 7) Main training loop\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        # Set model betas\n",
    "        probunet_model.beta_0 = beta_0\n",
    "        probunet_model.beta_1 = beta_1\n",
    "        probunet_model.beta_2 = beta_2\n",
    "\n",
    "        print(f\"Epoch {epoch}/{args.num_epochs} - beta_0: {probunet_model.beta_0:.4f}, \"\n",
    "              f\"beta_1: {probunet_model.beta_1:.4f}, beta_2: {probunet_model.beta_2:.4f}\")\n",
    "\n",
    "        # 7a) Train for one epoch (returns mean_crps, mean_kl, mean_kl2)\n",
    "        train_crps, train_kl, train_kl2 = tm.train_probunet_step(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_train,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            num_epochs=args.num_epochs,\n",
    "            device=args.device,       \n",
    "            ensemble_size=1    # how many samples per forward pass\n",
    "        )\n",
    "        train_crps_list.append(train_crps)\n",
    "        train_kl_list.append(train_kl)\n",
    "        train_kl2_list.append(train_kl2)\n",
    "\n",
    "        # 7b) Update betas after warmup\n",
    "        if epoch > warmup_epochs:\n",
    "            # beta_0 = 1.0 / (train_crps + 1e-7)\n",
    "            beta_0 = 1.0\n",
    "            beta_1 = 1.0 / (train_kl   + 1e-7)\n",
    "            # beta_2 = 1.0 / (train_kl2  + 1e-7)\n",
    "            beta_2 = 0.0\n",
    "        else:\n",
    "            beta_0, beta_1, beta_2 = 1.0, 0.0, 0.0\n",
    "\n",
    "        # 7c) Evaluate on validation data\n",
    "        val_crps, val_kl, val_kl2 = tm.eval_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_val,\n",
    "            device=args.device,\n",
    "            ensemble_size=1\n",
    "        )\n",
    "        val_crps_list.append(val_crps)\n",
    "        val_kl_list.append(val_kl)\n",
    "        val_kl2_list.append(val_kl2)\n",
    "\n",
    "        print(f\"[Train] CRPS={train_crps:.4f}, KL={train_kl:.4f}, KL2={train_kl2:.4f} | \"\n",
    "              f\"[Val] CRPS={val_crps:.4f}, KL={val_kl:.4f}, KL2={val_kl2:.4f}\")\n",
    "\n",
    "        # 7d) Example sampling from the model for sanity checks\n",
    "        test_batch = next(iter(dataloader_test_random))\n",
    "\n",
    "        # Residual predictions\n",
    "        residual_preds, (fig, axs) = tm.sample_residual_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_residuals.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig_difs, axs_difs = dataset_test.plot_residual_differences(\n",
    "            residual_preds=residual_preds,\n",
    "            timestamps_float=test_batch['timestamps_float'][:2],\n",
    "            epoch=epoch,\n",
    "            N=2, \n",
    "            num_samples=3\n",
    "        )\n",
    "        fig_difs.savefig(f\"{args.plotdir}/epoch{epoch}_res_difs.png\", dpi=300)\n",
    "        plt.close(fig_difs)\n",
    "\n",
    "        # Full reconstruction\n",
    "        samples, (fig, axs) = tm.sample_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_reconstructed.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # -- 8) Save final model weights\n",
    "    torch.save(probunet_model.state_dict(),\n",
    "               f\"{args.plotdir}/probunet_model_lat_dim_{probunet_model.latent_dim}.pth\")\n",
    "\n",
    "    # -- 9) Save losses for analysis\n",
    "    losses_to_save = {\n",
    "        \"train_crps\": train_crps_list,\n",
    "        \"train_kl\":   train_kl_list,\n",
    "        \"train_kl2\":  train_kl2_list,\n",
    "        \"val_crps\":   val_crps_list,\n",
    "        \"val_kl\":     val_kl_list,\n",
    "        \"val_kl2\":    val_kl2_list,\n",
    "    }\n",
    "    with open(f\"{args.plotdir}/losses.pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses_to_save, f)\n",
    "\n",
    "  \n",
    "    epochs = np.arange(1, args.num_epochs+1)\n",
    "    plt.plot(epochs, train_crps_list, label='Train CRPS')\n",
    "    plt.plot(epochs, val_crps_list,   label='Val CRPS', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('CRPS')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation CRPS')\n",
    "    plt.savefig(f\"{args.plotdir}/CRPS_curve.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_path, args, latent_dim, num_filters, model_channels, channel_mult):\n",
    "    \"\"\"\n",
    "    Load a trained ProbabilisticUNet model with specified latent_dim, num_filters, model_channels, and channel_mult.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model weights.\n",
    "        args: Arguments for loading.\n",
    "        latent_dim: Latent space dimension for the ProbabilisticUNet.\n",
    "        num_filters: List specifying the number of filters at each U-Net level.\n",
    "        model_channels: Base number of feature maps in the U-Net.\n",
    "        channel_mult: Multipliers for feature maps at each resolution level in the U-Net.\n",
    "\n",
    "    Returns:\n",
    "        Loaded model in evaluation mode.\n",
    "    \"\"\"\n",
    "    model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=latent_dim,\n",
    "        num_filters=num_filters,\n",
    "        model_channels=model_channels,\n",
    "        channel_mult=channel_mult,\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, dataloader_test, num_samples=20, save_dir=\"./test_predictions\"):\n",
    "    \"\"\"\n",
    "    Generate predictions for the entire test set and save the samples to .npy files.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ProbabilisticUNet model.\n",
    "        dataloader_test: DataLoader for the test set.\n",
    "        num_samples: Number of samples per test sample.\n",
    "        save_dir: Directory where .npy files will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    all_preds = []  # Store predictions for all test samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader_test, desc=\"Generating predictions\")):\n",
    "            inputs = batch['inputs'].to(args.device)  \n",
    "            lrinterp = batch['lrinterp'].to(args.device) \n",
    "            timestamps = batch['timestamps'].unsqueeze(dim=1).to(args.device)\n",
    "            hr_targets = batch['hr']  \n",
    "            \n",
    "            sample_preds = []  # Store 50 predictions for this batch\n",
    "            for _ in range(num_samples):\n",
    "                preds = model(inputs, t=timestamps, training=False)  \n",
    "                full_recon = dataloader_test.dataset.residual_to_hr(preds.cpu(), lrinterp.cpu())  \n",
    "                sample_preds.append(full_recon.numpy())  # Convert to NumPy array\n",
    "\n",
    "            # Stack predictions along a new axis (shape: [batch_size, 50, C, H, W])\n",
    "            sample_preds = np.stack(sample_preds, axis=1)\n",
    "            all_preds.append(sample_preds)\n",
    "\n",
    "    # Concatenate all batches (shape: [N, 50, C, H, W])\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    save_path = os.path.join(save_dir, \"predictions.npy\")\n",
    "    np.save(save_path, all_preds)  # Save to .npy file\n",
    "    print(f\"Predictions saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Testing model from ./results/plots/01/16/202511:40:11/probunet_model_lat_dim_16.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|                                                                                                                                      | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:24<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./test_predictions/16_16_Region_64_by_64/predictions.npy\n"
     ]
    }
   ],
   "source": [
    "args = tm.get_args()\n",
    "\n",
    "dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "\n",
    "model_configs = [\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/05/202521:51:55/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/05/202522:14:38/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/06/202500:40:45/probunet_model_lat_dim_8.pth\",\n",
    "    #     \"latent_dim\": 8,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/06/202501:00:11/probunet_model_lat_dim_16.pth\",\n",
    "    #     \"latent_dim\": 16,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202518:33:16/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202521:03:10/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202521:29:20/probunet_model_lat_dim_8.pth\",\n",
    "    #     \"latent_dim\": 8,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202522:01:12/probunet_model_lat_dim_16.pth\",\n",
    "    #     \"latent_dim\": 16,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202522:21:41/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/08/202512:38:04/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/08/202513:41:58/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    {\n",
    "        \"model_path\": \"./results/plots/01/16/202511:40:11/probunet_model_lat_dim_16.pth\",\n",
    "        \"latent_dim\": 16,\n",
    "        \"num_filters\": [16, 64, 128, 256],\n",
    "        \"model_channels\": 16,\n",
    "        \"channel_mult\": [1, 4, 8, 16]\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "for config in model_configs:\n",
    "    print(f\"Testing model from {config['model_path']}\")\n",
    "    \n",
    "    # Load the model\n",
    "    probunet_model = load_model(\n",
    "        model_path=config[\"model_path\"],\n",
    "        args=args,\n",
    "        latent_dim=config[\"latent_dim\"],\n",
    "        num_filters=config[\"num_filters\"],\n",
    "        model_channels=config[\"model_channels\"],\n",
    "        channel_mult=config[\"channel_mult\"]\n",
    "    )\n",
    "\n",
    "    # Generate predictions and save them\n",
    "    generate_samples(probunet_model, dataloader_test, num_samples=20, save_dir=f\"./test_predictions/{config['latent_dim']}_{config['model_channels']}_Region_64_by_64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
