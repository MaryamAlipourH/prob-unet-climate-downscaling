{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "# from prob_unet_utils import plot_losses, plot_losses_mae\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Probabilistic Unet Latent dim: 16\n",
      "Epoch 1/10 - beta_0: 1.0000, beta_1: 0.0000, beta_2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/10:   0%|          | 0/343 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/10:   5%|▍         | 16/343 [00:04<01:27,  3.73it/s, Loss: 0.2439]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 130\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - beta_0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobunet_model\u001b[38;5;241m.\u001b[39mbeta_0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobunet_model\u001b[38;5;241m.\u001b[39mbeta_1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, beta_2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobunet_model\u001b[38;5;241m.\u001b[39mbeta_2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# 7a) Train for one epoch (returns mean_crps, mean_kl, mean_kl2)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m train_crps, train_kl, train_kl2 \u001b[38;5;241m=\u001b[39m tm\u001b[38;5;241m.\u001b[39mtrain_probunet_step(\n\u001b[1;32m    131\u001b[0m     model\u001b[38;5;241m=\u001b[39mprobunet_model,\n\u001b[1;32m    132\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mdataloader_train,\n\u001b[1;32m    133\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m    134\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    135\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_epochs,\n\u001b[1;32m    136\u001b[0m     device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice,        \n\u001b[1;32m    137\u001b[0m     ensemble_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m    \u001b[38;5;66;03m# how many samples per forward pass\u001b[39;00m\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m train_crps_list\u001b[38;5;241m.\u001b[39mappend(train_crps)\n\u001b[1;32m    140\u001b[0m train_kl_list\u001b[38;5;241m.\u001b[39mappend(train_kl)\n",
      "File \u001b[0;32m~/prob-unet-mds/train_prob_unet_model.py:195\u001b[0m, in \u001b[0;36mtrain_probunet_step\u001b[0;34m(model, dataloader, optimizer, epoch, num_epochs, device, ensemble_size)\u001b[0m\n\u001b[1;32m    191\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamps\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Forward pass: get total ELBO-like loss (with afCRPS),\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# plus the scalar CRPS, KL, KL2 for logging\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m loss, recon_list, kl_div, kl_div2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39melbo(\n\u001b[1;32m    196\u001b[0m     inputs, targets, timestamps, \n\u001b[1;32m    197\u001b[0m     M\u001b[38;5;241m=\u001b[39mensemble_size \n\u001b[1;32m    198\u001b[0m )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Backprop and optimize\u001b[39;00m\n\u001b[1;32m    201\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/prob-unet-mds/prob_unet.py:284\u001b[0m, in \u001b[0;36mProbabilisticUNet.elbo\u001b[0;34m(self, x, target, t, M, alpha)\u001b[0m\n\u001b[1;32m    282\u001b[0m unet_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(x)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_latent_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior(x)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposterior_latent_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposterior(x, target)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# 2) Draw M samples from the posterior\u001b[39;00m\n\u001b[1;32m    287\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/prob-unet-mds/prob_unet.py:84\u001b[0m, in \u001b[0;36mAxisAlignedConvGaussian.forward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m     81\u001b[0m log_sigma \u001b[38;5;241m=\u001b[39m log_sigma\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Create a Normal distribution with the computed parameters\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m dist \u001b[38;5;241m=\u001b[39m Independent(Normal(loc\u001b[38;5;241m=\u001b[39mmu, scale\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mexp(log_sigma) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-7\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(batch_shape, validate_args\u001b[38;5;241m=\u001b[39mvalidate_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.12/site-packages/torch/distributions/distribution.py:67\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     65\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m---> 67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training model with afcrps\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "# from prob_unet_utils import plot_losses, plot_losses_mae\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed) \n",
    "        np.random.seed(seed)  \n",
    "        torch.manual_seed(seed) \n",
    "        torch.cuda.manual_seed(seed)  \n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "        torch.backends.cudnn.deterministic = True  \n",
    "        torch.backends.cudnn.benchmark = False  \n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # -- 1) Set seed for reproducibility\n",
    "    set_seed(42)  \n",
    "\n",
    "    # -- 2) Importing all required arguments\n",
    "    args = tm.get_args()\n",
    "    args.lowres_scale = 16\n",
    "    args.batch_size = 32\n",
    "    args.num_epochs = 10\n",
    "\n",
    "    # Initialize the Probabilistic UNet model\n",
    "    probunet_model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=16,\n",
    "        num_filters=[32, 64, 128, 256],\n",
    "        model_channels=32,\n",
    "        channel_mult=[1, 2, 4, 8],\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "\n",
    "    # -- 3) Prepare datasets\n",
    "    dataset_train = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_train,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_val = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_val,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "    # -- 4) Build DataLoaders\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test_random = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # -- 5) Define optimizer\n",
    "    optimizer = args.optimizer(params=probunet_model.parameters(), lr=args.lr)\n",
    "    # Example alternative:\n",
    "    # optimizer = torch.optim.Adam(probunet_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "    # -- 6) We track CRPS, KL, KL2 each epoch for train/val\n",
    "    train_crps_list, train_kl_list, train_kl2_list = [], [], []\n",
    "    val_crps_list,   val_kl_list,   val_kl2_list   = [], [], []\n",
    "\n",
    "    # For convenience, we keep your adaptive betas:\n",
    "    beta_0 = 1.0\n",
    "    beta_1 = 0.0\n",
    "    warmup_epochs = 2\n",
    "\n",
    "    print(f\"Probabilistic Unet Latent dim: {probunet_model.latent_dim}\")\n",
    "\n",
    "    # -- 7) Main training loop\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        # Set model betas\n",
    "        probunet_model.beta_0 = beta_0\n",
    "        probunet_model.beta_1 = beta_1\n",
    "\n",
    "        print(f\"Epoch {epoch}/{args.num_epochs} - beta_0: {probunet_model.beta_0:.4f}, \"\n",
    "              f\"beta_1: {probunet_model.beta_1:.4f}\")\n",
    "\n",
    "        # 7a) Train for one epoch (returns mean_crps, mean_kl, mean_kl2)\n",
    "        train_crps, train_kl, train_kl2 = tm.train_probunet_step(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_train,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            num_epochs=args.num_epochs,\n",
    "            device=args.device,        \n",
    "            ensemble_size=15    # how many samples per forward pass\n",
    "        )\n",
    "        train_crps_list.append(train_crps)\n",
    "        train_kl_list.append(train_kl)\n",
    "        train_kl2_list.append(train_kl2)\n",
    "\n",
    "        # 7b) Update betas after warmup\n",
    "        if epoch > warmup_epochs:\n",
    "            # beta_0 = 1.0 / (train_crps + 1e-7)\n",
    "            beta_0 = 1.0\n",
    "            beta_1 = 1.0 / (train_kl   + 1e-7)\n",
    "        else:\n",
    "            beta_0, beta_1= 1.0, 0.0\n",
    "\n",
    "        # 7c) Evaluate on validation data\n",
    "        val_crps, val_kl = tm.eval_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_val,\n",
    "            device=args.device,\n",
    "            ensemble_size=5\n",
    "        )\n",
    "        val_crps_list.append(val_crps)\n",
    "        val_kl_list.append(val_kl)\n",
    "\n",
    "        print(f\"[Train] CRPS={train_crps:.4f}, KL={train_kl:.4f}| \"\n",
    "              f\"[Val] CRPS={val_crps:.4f}, KL={val_kl:.4f}\")\n",
    "\n",
    "        # 7d) Example sampling from the model for sanity checks\n",
    "        test_batch = next(iter(dataloader_test_random))\n",
    "\n",
    "        # Residual predictions\n",
    "        residual_preds, (fig, axs) = tm.sample_residual_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_residuals.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig_difs, axs_difs = dataset_test.plot_residual_differences(\n",
    "            residual_preds=residual_preds,\n",
    "            timestamps_float=test_batch['timestamps_float'][:2],\n",
    "            epoch=epoch,\n",
    "            N=2, \n",
    "            num_samples=3\n",
    "        )\n",
    "        fig_difs.savefig(f\"{args.plotdir}/epoch{epoch}_res_difs.png\", dpi=300)\n",
    "        plt.close(fig_difs)\n",
    "\n",
    "        # Full reconstruction\n",
    "        samples, (fig, axs) = tm.sample_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_reconstructed.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # -- 8) Save final model weights\n",
    "    torch.save(probunet_model.state_dict(),\n",
    "               f\"{args.plotdir}/probunet_model_lat_dim_{probunet_model.latent_dim}.pth\")\n",
    "\n",
    "    # -- 9) Save losses for analysis\n",
    "    losses_to_save = {\n",
    "        \"train_crps\": train_crps_list,\n",
    "        \"train_kl\":   train_kl_list,\n",
    "        \"val_crps\":   val_crps_list,\n",
    "        \"val_kl\":     val_kl_list,\n",
    "    }\n",
    "    with open(f\"{args.plotdir}/losses.pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses_to_save, f)\n",
    "\n",
    "  \n",
    "    epochs = np.arange(1, args.num_epochs+1)\n",
    "    plt.plot(epochs, train_crps_list, label='Train CRPS')\n",
    "    plt.plot(epochs, val_crps_list,   label='Val CRPS', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('CRPS')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation CRPS')\n",
    "    plt.savefig(f\"{args.plotdir}/CRPS_curve.png\", dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Probabilistic Unet Latent dim: 16\n",
      "Epoch 1/10 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/10:   0%|          | 1/343 [00:00<01:06,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/10: 100%|██████████| 343/343 [00:57<00:00,  5.99it/s, Loss: 0.1232]\n",
      ":: Evaluation :::   2%|▏         | 2/92 [00:00<00:05, 16.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Evaluation ::: 100%|██████████| 92/92 [00:05<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] recon_loss=0.1687, KL=1406.9236, WMSE=0.0016, MSSSIM=0.1687 | [Val] recon_loss=0.1134, KL=1476.5981, WMSE=0.0010, MSSSIM=0.1134\n",
      "Epoch 2/10 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 2/10: 100%|██████████| 343/343 [00:57<00:00,  6.02it/s, Loss: 0.1168]\n",
      ":: Evaluation ::: 100%|██████████| 92/92 [00:05<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] recon_loss=0.0938, KL=1420.8607, WMSE=0.0009, MSSSIM=0.0938 | [Val] recon_loss=0.0980, KL=1178.9982, WMSE=0.0009, MSSSIM=0.0980\n",
      "Epoch 3/10 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 3/10:  98%|█████████▊| 335/343 [00:55<00:01,  6.02it/s, Loss: 0.0826]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 117\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - beta_0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobunet_model\u001b[38;5;241m.\u001b[39mbeta_0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobunet_model\u001b[38;5;241m.\u001b[39mbeta_1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# 7a) Train for one epoch (returns mean_crps, mean_kl)\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m train_crps, train_kl, train_wmse, train_msssim \u001b[38;5;241m=\u001b[39m tm\u001b[38;5;241m.\u001b[39mtrain_probunet_step(\n\u001b[1;32m    118\u001b[0m     model\u001b[38;5;241m=\u001b[39mprobunet_model,\n\u001b[1;32m    119\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mdataloader_train,\n\u001b[1;32m    120\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m    121\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    122\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_epochs,\n\u001b[1;32m    123\u001b[0m     device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice,       \n\u001b[1;32m    124\u001b[0m     ensemble_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m    \u001b[38;5;66;03m# how many samples per forward pass\u001b[39;00m\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m train_crps_list\u001b[38;5;241m.\u001b[39mappend(train_crps)\n\u001b[1;32m    127\u001b[0m train_kl_list\u001b[38;5;241m.\u001b[39mappend(train_kl)\n",
      "File \u001b[0;32m~/prob-unet-mds/train_prob_unet_model.py:206\u001b[0m, in \u001b[0;36mtrain_probunet_step\u001b[0;34m(model, dataloader, optimizer, epoch, num_epochs, device, ensemble_size)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# crps_list is just [crps_value], so record it\u001b[39;00m\n\u001b[1;32m    205\u001b[0m recon_vals\u001b[38;5;241m.\u001b[39mappend(recon_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 206\u001b[0m kl_vals\u001b[38;5;241m.\u001b[39mappend(kl_div\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    207\u001b[0m wmse_vals\u001b[38;5;241m.\u001b[39mappend(wmse)\n\u001b[1;32m    208\u001b[0m msssim_vals\u001b[38;5;241m.\u001b[39mappend(msssim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training model with wmse-ms-ssim\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed) \n",
    "        np.random.seed(seed)  \n",
    "        torch.manual_seed(seed) \n",
    "        torch.cuda.manual_seed(seed)  \n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "        torch.backends.cudnn.deterministic = True  \n",
    "        torch.backends.cudnn.benchmark = False  \n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # -- 1) Set seed for reproducibility\n",
    "    set_seed(42)  \n",
    "\n",
    "    # -- 2) Importing all required arguments\n",
    "    args = tm.get_args()\n",
    "    args.lowres_scale = 16\n",
    "    args.batch_size = 32\n",
    "    args.num_epochs = 10\n",
    "\n",
    "    # Initialize the Probabilistic UNet model\n",
    "    probunet_model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=16,\n",
    "        num_filters=[32, 64, 128, 256],\n",
    "        model_channels=32,\n",
    "        channel_mult=[1, 2, 4, 8],\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "\n",
    "    # -- 3) Prepare datasets\n",
    "    dataset_train = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_train,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_val = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_val,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "    # -- 4) Build DataLoaders\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test_random = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # -- 5) Define optimizer\n",
    "    optimizer = args.optimizer(params=probunet_model.parameters(), lr=args.lr)\n",
    "    # Example alternative:\n",
    "    # optimizer = torch.optim.Adam(probunet_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "    # -- 6) We track CRPS, KL each epoch for train/val\n",
    "    train_crps_list, train_kl_list = [], []\n",
    "    val_crps_list,   val_kl_list = [], []\n",
    "\n",
    "    # For convenience, we keep your adaptive betas:\n",
    "    beta_0 = 1.0\n",
    "    beta_1 = 0.0\n",
    "    warmup_epochs = 2\n",
    "\n",
    "    print(f\"Probabilistic Unet Latent dim: {probunet_model.latent_dim}\")\n",
    "\n",
    "    # -- 7) Main training loop\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        # Set model betas\n",
    "        probunet_model.beta_0 = beta_0\n",
    "        probunet_model.beta_1 = beta_1\n",
    "\n",
    "        print(f\"Epoch {epoch}/{args.num_epochs} - beta_0: {probunet_model.beta_0:.4f}, \"\n",
    "              f\"beta_1: {probunet_model.beta_1:.4f}\")\n",
    "\n",
    "        # 7a) Train for one epoch (returns mean_crps, mean_kl)\n",
    "        train_crps, train_kl, train_wmse, train_msssim = tm.train_probunet_step(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_train,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            num_epochs=args.num_epochs,\n",
    "            device=args.device,       \n",
    "            ensemble_size=1    # how many samples per forward pass\n",
    "        )\n",
    "        train_crps_list.append(train_crps)\n",
    "        train_kl_list.append(train_kl)\n",
    "\n",
    "        # 7b) Update betas after warmup\n",
    "        if epoch > warmup_epochs:\n",
    "            beta_0 = 1.0 / (train_crps + 1e-7)\n",
    "            # beta_0 = 1.0\n",
    "            beta_1 = 1.0 / (train_kl   + 1e-7)\n",
    "        else:\n",
    "            beta_0, beta_1 = 1.0, 0.0\n",
    "\n",
    "        # 7c) Evaluate on validation data\n",
    "        val_crps, val_kl, val_wmse, val_msssim = tm.eval_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_val,\n",
    "            device=args.device,\n",
    "            ensemble_size=1\n",
    "        )\n",
    "        val_crps_list.append(val_crps)\n",
    "        val_kl_list.append(val_kl)\n",
    "\n",
    "        print(f\"[Train] recon_loss={train_crps:.4f}, KL={train_kl:.4f}, WMSE={train_wmse:.4f}, MSSSIM={train_msssim:.4f} | \"\n",
    "              f\"[Val] recon_loss={val_crps:.4f}, KL={val_kl:.4f}, WMSE={val_wmse:.4f}, MSSSIM={val_msssim:.4f}\")\n",
    "        # 7d) Example sampling from the model for sanity checks\n",
    "        test_batch = next(iter(dataloader_test_random))\n",
    "\n",
    "        # Residual predictions\n",
    "        residual_preds, (fig, axs) = tm.sample_residual_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_residuals.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig_difs, axs_difs = dataset_test.plot_residual_differences(\n",
    "            residual_preds=residual_preds,\n",
    "            timestamps_float=test_batch['timestamps_float'][:2],\n",
    "            epoch=epoch,\n",
    "            N=2, \n",
    "            num_samples=3\n",
    "        )\n",
    "        fig_difs.savefig(f\"{args.plotdir}/epoch{epoch}_res_difs.png\", dpi=300)\n",
    "        plt.close(fig_difs)\n",
    "\n",
    "        # Full reconstruction\n",
    "        samples, (fig, axs) = tm.sample_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_reconstructed.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # -- 8) Save final model weights\n",
    "    torch.save(probunet_model.state_dict(),\n",
    "               f\"{args.plotdir}/probunet_model_lat_dim_{probunet_model.latent_dim}.pth\")\n",
    "\n",
    "    # -- 9) Save losses for analysis\n",
    "    losses_to_save = {\n",
    "        \"train_crps\": train_crps_list,\n",
    "        \"train_kl\":   train_kl_list,\n",
    "        \"val_crps\":   val_crps_list,\n",
    "        \"val_kl\":     val_kl_list,\n",
    "    }\n",
    "    with open(f\"{args.plotdir}/losses.pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses_to_save, f)\n",
    "\n",
    "  \n",
    "    epochs = np.arange(1, args.num_epochs+1)\n",
    "    plt.plot(epochs, train_crps_list, label='Train CRPS')\n",
    "    plt.plot(epochs, val_crps_list,   label='Val CRPS', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('CRPS')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation CRPS')\n",
    "    plt.savefig(f\"{args.plotdir}/CRPS_curve.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_path, args, latent_dim, num_filters, model_channels, channel_mult):\n",
    "    \"\"\"\n",
    "    Load a trained ProbabilisticUNet model with specified latent_dim, num_filters, model_channels, and channel_mult.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model weights.\n",
    "        args: Arguments for loading.\n",
    "        latent_dim: Latent space dimension for the ProbabilisticUNet.\n",
    "        num_filters: List specifying the number of filters at each U-Net level.\n",
    "        model_channels: Base number of feature maps in the U-Net.\n",
    "        channel_mult: Multipliers for feature maps at each resolution level in the U-Net.\n",
    "\n",
    "    Returns:\n",
    "        Loaded model in evaluation mode.\n",
    "    \"\"\"\n",
    "    model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=latent_dim,\n",
    "        num_filters=num_filters,\n",
    "        model_channels=model_channels,\n",
    "        channel_mult=channel_mult,\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, dataloader_test, num_samples=20, save_dir=\"./test_predictions\"):\n",
    "    \"\"\"\n",
    "    Generate predictions for the entire test set and save the samples to .npy files.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ProbabilisticUNet model.\n",
    "        dataloader_test: DataLoader for the test set.\n",
    "        num_samples: Number of samples per test sample.\n",
    "        save_dir: Directory where .npy files will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    all_preds = []  # Store predictions for all test samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader_test, desc=\"Generating predictions\")):\n",
    "            inputs = batch['inputs'].to(args.device)  \n",
    "            lrinterp = batch['lrinterp'].to(args.device) \n",
    "            timestamps = batch['timestamps'].unsqueeze(dim=1).to(args.device)\n",
    "            hr_targets = batch['hr']  \n",
    "            \n",
    "            sample_preds = []  # Store 50 predictions for this batch\n",
    "            for _ in range(num_samples):\n",
    "                preds = model(inputs, t=timestamps, training=False)  \n",
    "                full_recon = dataloader_test.dataset.residual_to_hr(preds.cpu(), lrinterp.cpu())  \n",
    "                sample_preds.append(full_recon.numpy())  # Convert to NumPy array\n",
    "\n",
    "            # Stack predictions along a new axis (shape: [batch_size, 50, C, H, W])\n",
    "            sample_preds = np.stack(sample_preds, axis=1)\n",
    "            all_preds.append(sample_preds)\n",
    "\n",
    "    # Concatenate all batches (shape: [N, 50, C, H, W])\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    save_path = os.path.join(save_dir, \"predictions.npy\")\n",
    "    np.save(save_path, all_preds)  # Save to .npy file\n",
    "    print(f\"Predictions saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Testing model from ./results/plots/01/16/202511:40:11/probunet_model_lat_dim_16.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|                                                                                                                                      | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:24<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./test_predictions/16_16_Region_64_by_64/predictions.npy\n"
     ]
    }
   ],
   "source": [
    "args = tm.get_args()\n",
    "\n",
    "dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "\n",
    "model_configs = [\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/05/202521:51:55/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/05/202522:14:38/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/06/202500:40:45/probunet_model_lat_dim_8.pth\",\n",
    "    #     \"latent_dim\": 8,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/06/202501:00:11/probunet_model_lat_dim_16.pth\",\n",
    "    #     \"latent_dim\": 16,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202518:33:16/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202521:03:10/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202521:29:20/probunet_model_lat_dim_8.pth\",\n",
    "    #     \"latent_dim\": 8,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202522:01:12/probunet_model_lat_dim_16.pth\",\n",
    "    #     \"latent_dim\": 16,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202522:21:41/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/08/202512:38:04/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/08/202513:41:58/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    {\n",
    "        \"model_path\": \"./results/plots/01/16/202511:40:11/probunet_model_lat_dim_16.pth\",\n",
    "        \"latent_dim\": 16,\n",
    "        \"num_filters\": [16, 64, 128, 256],\n",
    "        \"model_channels\": 16,\n",
    "        \"channel_mult\": [1, 4, 8, 16]\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "for config in model_configs:\n",
    "    print(f\"Testing model from {config['model_path']}\")\n",
    "    \n",
    "    # Load the model\n",
    "    probunet_model = load_model(\n",
    "        model_path=config[\"model_path\"],\n",
    "        args=args,\n",
    "        latent_dim=config[\"latent_dim\"],\n",
    "        num_filters=config[\"num_filters\"],\n",
    "        model_channels=config[\"model_channels\"],\n",
    "        channel_mult=config[\"channel_mult\"]\n",
    "    )\n",
    "\n",
    "    # Generate predictions and save them\n",
    "    generate_samples(probunet_model, dataloader_test, num_samples=20, save_dir=f\"./test_predictions/{config['latent_dim']}_{config['model_channels']}_Region_64_by_64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
