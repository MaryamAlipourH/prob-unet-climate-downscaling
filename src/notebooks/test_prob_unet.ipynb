{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "# from prob_unet_utils import plot_losses, plot_losses_mae\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Probabilistic Unet Latent dim: 16\n",
      "Epoch 1/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/30:   0%|          | 0/685 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/30: 100%|██████████| 685/685 [02:59<00:00,  3.81it/s, Loss: 0.1325]\n",
      ":: Evaluation :::   1%|          | 1/103 [00:00<00:20,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1476, KL=1790.5485| [Val] CRPS=0.1347, KL=2775.1663\n",
      "Epoch 2/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 2/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1394]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1256, KL=2582.2215| [Val] CRPS=0.1284, KL=2572.4964\n",
      "Epoch 3/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 3/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1236]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1217, KL=2454.7891| [Val] CRPS=0.1269, KL=2850.2980\n",
      "Epoch 4/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 4/30: 100%|██████████| 685/685 [02:58<00:00,  3.85it/s, Loss: 0.1146]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1194, KL=2239.7311| [Val] CRPS=0.1249, KL=2108.2311\n",
      "Epoch 5/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 5/30: 100%|██████████| 685/685 [02:58<00:00,  3.83it/s, Loss: 0.1268]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1178, KL=1901.8787| [Val] CRPS=0.1237, KL=1883.3967\n",
      "Epoch 6/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 6/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1188]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1165, KL=1770.3983| [Val] CRPS=0.1223, KL=2709.7864\n",
      "Epoch 7/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 7/30: 100%|██████████| 685/685 [02:58<00:00,  3.85it/s, Loss: 0.1151]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1154, KL=1899.7130| [Val] CRPS=0.1227, KL=1901.7465\n",
      "Epoch 8/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 8/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1198]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1143, KL=1678.5079| [Val] CRPS=0.1214, KL=1503.0295\n",
      "Epoch 9/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 9/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1095]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1134, KL=1683.9382| [Val] CRPS=0.1209, KL=1893.5939\n",
      "Epoch 10/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 10/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1105]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1125, KL=1597.7627| [Val] CRPS=0.1205, KL=1802.4214\n",
      "Epoch 11/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 11/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1126]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1116, KL=1526.5787| [Val] CRPS=0.1201, KL=1633.6973\n",
      "Epoch 12/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 12/30: 100%|██████████| 685/685 [02:59<00:00,  3.82it/s, Loss: 0.1058]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1108, KL=1404.7101| [Val] CRPS=0.1198, KL=1701.8596\n",
      "Epoch 13/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 13/30: 100%|██████████| 685/685 [02:59<00:00,  3.82it/s, Loss: 0.1103]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1100, KL=1377.8187| [Val] CRPS=0.1204, KL=1638.4421\n",
      "Epoch 14/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 14/30: 100%|██████████| 685/685 [02:58<00:00,  3.83it/s, Loss: 0.1051]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1093, KL=1466.7424| [Val] CRPS=0.1201, KL=2138.6040\n",
      "Epoch 15/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 15/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1058]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1085, KL=1172.1625| [Val] CRPS=0.1197, KL=1648.1453\n",
      "Epoch 16/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 16/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1121]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1078, KL=1102.5997| [Val] CRPS=0.1200, KL=1356.9555\n",
      "Epoch 17/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 17/30: 100%|██████████| 685/685 [02:58<00:00,  3.84it/s, Loss: 0.1072]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1070, KL=1138.4439| [Val] CRPS=0.1200, KL=1375.0674\n",
      "Epoch 18/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 18/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.1118]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1064, KL=975.4800| [Val] CRPS=0.1194, KL=1487.2089\n",
      "Epoch 19/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 19/30: 100%|██████████| 685/685 [02:57<00:00,  3.86it/s, Loss: 0.1035]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1058, KL=1045.3667| [Val] CRPS=0.1204, KL=1332.9770\n",
      "Epoch 20/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 20/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.1091]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1051, KL=838.9338| [Val] CRPS=0.1194, KL=897.7815\n",
      "Epoch 21/30 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 21/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.1109]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1046, KL=841.8784| [Val] CRPS=0.1204, KL=781.3664\n",
      "  → Annealing progress: 10.00% | beta_1 = 0.0001\n",
      "Epoch 22/30 - beta_0: 1.0000, beta_1: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 22/30: 100%|██████████| 685/685 [02:58<00:00,  3.85it/s, Loss: 0.1009]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1041, KL=1.1062| [Val] CRPS=0.1206, KL=0.0896\n",
      "  → Annealing progress: 20.00% | beta_1 = 0.0002\n",
      "Epoch 23/30 - beta_0: 1.0000, beta_1: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 23/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.0953]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1035, KL=0.0649| [Val] CRPS=0.1198, KL=0.0506\n",
      "  → Annealing progress: 30.00% | beta_1 = 0.0003\n",
      "Epoch 24/30 - beta_0: 1.0000, beta_1: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 24/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.1043]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1030, KL=0.0451| [Val] CRPS=0.1205, KL=0.0892\n",
      "  → Annealing progress: 40.00% | beta_1 = 0.0004\n",
      "Epoch 25/30 - beta_0: 1.0000, beta_1: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 25/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.0957]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1024, KL=0.0367| [Val] CRPS=0.1199, KL=0.0302\n",
      "  → Annealing progress: 50.00% | beta_1 = 0.0005\n",
      "Epoch 26/30 - beta_0: 1.0000, beta_1: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 26/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.1061]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1020, KL=0.0266| [Val] CRPS=0.1208, KL=0.0187\n",
      "  → Annealing progress: 60.00% | beta_1 = 0.0006\n",
      "Epoch 27/30 - beta_0: 1.0000, beta_1: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 27/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.1083]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1015, KL=0.0181| [Val] CRPS=0.1203, KL=0.0188\n",
      "  → Annealing progress: 70.00% | beta_1 = 0.0007\n",
      "Epoch 28/30 - beta_0: 1.0000, beta_1: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 28/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.1019]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1011, KL=0.0188| [Val] CRPS=0.1202, KL=0.0118\n",
      "  → Annealing progress: 80.00% | beta_1 = 0.0008\n",
      "Epoch 29/30 - beta_0: 1.0000, beta_1: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 29/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.0982]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1007, KL=0.0142| [Val] CRPS=0.1209, KL=0.0169\n",
      "  → Annealing progress: 90.00% | beta_1 = 0.0009\n",
      "Epoch 30/30 - beta_0: 1.0000, beta_1: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 30/30: 100%|██████████| 685/685 [02:57<00:00,  3.85it/s, Loss: 0.1086]\n",
      ":: Evaluation ::: 100%|██████████| 103/103 [00:06<00:00, 16.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] CRPS=0.1002, KL=0.0157| [Val] CRPS=0.1215, KL=0.0267\n",
      "  → Annealing progress: 100.00% | beta_1 = 0.0010\n",
      "\n",
      "============================================================\n",
      "ANALYZING RESIDUAL CONTRIBUTION\n",
      "============================================================\n",
      "Computing statistics for standardization\n",
      "\n",
      "[ANALYSIS] Residual Contribution:\n",
      "  Mean |predicted_residual|: 0.3149\n",
      "  Mean |true_residual|: 1.3834\n",
      "  Error (lrinterp only): 1.3834\n",
      "  Error (lrinterp + model): 0.9807\n",
      "  Improvement: 0.4027\n",
      "  Improvement %: 29.11%\n"
     ]
    }
   ],
   "source": [
    "# Training model with afcrps\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "# from prob_unet_utils import plot_losses, plot_losses_mae\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed) \n",
    "        np.random.seed(seed)  \n",
    "        torch.manual_seed(seed) \n",
    "        torch.cuda.manual_seed(seed)  \n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "        torch.backends.cudnn.deterministic = True  \n",
    "        torch.backends.cudnn.benchmark = False  \n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # -- 1) Set seed for reproducibility\n",
    "    set_seed(42)  \n",
    "\n",
    "    # -- 2) Importing all required arguments\n",
    "    args = tm.get_args()\n",
    "    args.lowres_scale = 16\n",
    "    args.batch_size = 32\n",
    "    args.num_epochs = 30\n",
    "    args.years_train = range(1960, 2020)\n",
    "    args.years_val = range(2021, 2030)\n",
    "\n",
    "    # Initialize the Probabilistic UNet model\n",
    "    probunet_model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=16,\n",
    "        num_filters=[32, 64, 128, 256],\n",
    "        model_channels=32,\n",
    "        channel_mult=[1, 2, 4, 8],\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "\n",
    "    # -- 3) Prepare datasets\n",
    "    dataset_train = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_train,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_val = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_val,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "    # -- 4) Build DataLoaders\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test_random = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # -- 5) Define optimizer\n",
    "    optimizer = args.optimizer(params=probunet_model.parameters(), lr=args.lr)\n",
    "    # Example alternative:\n",
    "    # optimizer = torch.optim.Adam(probunet_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "    # -- 6) We track CRPS, KL each epoch for train/val\n",
    "    train_crps_list, train_kl_list = [], []\n",
    "    val_crps_list,   val_kl_list = [], []\n",
    "\n",
    "    # For convenience, we keep your adaptive betas:\n",
    "    beta_0 = 1.0\n",
    "    beta_1 = 0.0\n",
    "    warmup_epochs = 20\n",
    "    max_beta_1 = 0.001\n",
    "\n",
    "    print(f\"Probabilistic Unet Latent dim: {probunet_model.latent_dim}\")\n",
    "\n",
    "    # -- 7) Main training loop\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        # Set model betas\n",
    "        probunet_model.beta_0 = beta_0\n",
    "        probunet_model.beta_1 = beta_1\n",
    "\n",
    "        print(f\"Epoch {epoch}/{args.num_epochs} - beta_0: {probunet_model.beta_0:.4f}, \"\n",
    "              f\"beta_1: {probunet_model.beta_1:.4f}\")\n",
    "\n",
    "        # 7a) Train for one epoch (returns mean_crps, mean_kl)\n",
    "        train_crps, train_kl = tm.train_probunet_step(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_train,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            num_epochs=args.num_epochs,\n",
    "            device=args.device,        \n",
    "            ensemble_size=15    # how many samples per forward pass\n",
    "        )\n",
    "        train_crps_list.append(train_crps)\n",
    "        train_kl_list.append(train_kl)\n",
    "\n",
    "        # 7b) Evaluate on validation data\n",
    "        val_crps, val_kl = tm.eval_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_val,\n",
    "            device=args.device,\n",
    "            ensemble_size=5\n",
    "        )\n",
    "        val_crps_list.append(val_crps)\n",
    "        val_kl_list.append(val_kl)\n",
    "\n",
    "        print(f\"[Train] CRPS={train_crps:.4f}, KL={train_kl:.4f}| \"\n",
    "              f\"[Val] CRPS={val_crps:.4f}, KL={val_kl:.4f}\")\n",
    "        \n",
    "        # 7c) Update betas with gradual annealing\n",
    "        if epoch <= warmup_epochs:\n",
    "            # Warmup phase: no KL penalty\n",
    "            beta_0, beta_1 = 1.0, 0.0\n",
    "        else:\n",
    "            # Annealing phase: gradually increase beta_1 from 0 to max_beta_1\n",
    "            # Progress goes from 0 (just after warmup) to 1 (at final epoch)\n",
    "            total_annealing_epochs = args.num_epochs - warmup_epochs\n",
    "            current_annealing_epoch = epoch - warmup_epochs\n",
    "            progress = min(current_annealing_epoch / total_annealing_epochs, 1.0)\n",
    "            \n",
    "            beta_0 = 1.0\n",
    "            beta_1 = progress * max_beta_1\n",
    "            \n",
    "            print(f\"  → Annealing progress: {progress:.2%} | beta_1 = {beta_1:.4f}\")\n",
    "\n",
    "        # 7d) Example sampling from the model for sanity checks\n",
    "        test_batch = next(iter(dataloader_test_random))\n",
    "\n",
    "        # Residual predictions\n",
    "        residual_preds, (fig, axs) = tm.sample_residual_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_residuals.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig_difs, axs_difs = dataset_test.plot_residual_differences(\n",
    "            residual_preds=residual_preds,\n",
    "            timestamps_float=test_batch['timestamps_float'][:2],\n",
    "            epoch=epoch,\n",
    "            N=2, \n",
    "            num_samples=3\n",
    "        )\n",
    "        fig_difs.savefig(f\"{args.plotdir}/epoch{epoch}_res_difs.png\", dpi=300)\n",
    "        plt.close(fig_difs)\n",
    "\n",
    "        # Full reconstruction\n",
    "        samples, (fig, axs) = tm.sample_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_reconstructed.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # -- 8) Save final model weights\n",
    "    torch.save(probunet_model.state_dict(),\n",
    "               f\"{args.plotdir}/probunet_model_lat_dim_{probunet_model.latent_dim}.pth\")\n",
    "\n",
    "    # -- 9) Save losses for analysis\n",
    "    losses_to_save = {\n",
    "        \"train_crps\": train_crps_list,\n",
    "        \"train_kl\":   train_kl_list,\n",
    "        \"val_crps\":   val_crps_list,\n",
    "        \"val_kl\":     val_kl_list,\n",
    "    }\n",
    "    with open(f\"{args.plotdir}/losses.pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses_to_save, f)\n",
    "\n",
    "    # -- NEW: Analyze residual contribution\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYZING RESIDUAL CONTRIBUTION\")\n",
    "    print(\"=\"*60)\n",
    "    tm.analyze_residual_contribution(\n",
    "        model=probunet_model,\n",
    "        dataloader=dataloader_test,\n",
    "        device=args.device,\n",
    "        num_samples=20\n",
    "    )\n",
    "  \n",
    "    epochs = np.arange(1, args.num_epochs+1)\n",
    "    plt.plot(epochs, train_crps_list, label='Train CRPS')\n",
    "    plt.plot(epochs, val_crps_list,   label='Val CRPS', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('CRPS')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation CRPS')\n",
    "    plt.savefig(f\"{args.plotdir}/CRPS_curve.png\", dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Probabilistic Unet Latent dim: 16\n",
      "Epoch 1/10 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/10:   0%|          | 1/343 [00:00<01:06,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/10: 100%|██████████| 343/343 [00:57<00:00,  5.99it/s, Loss: 0.1232]\n",
      ":: Evaluation :::   2%|▏         | 2/92 [00:00<00:05, 16.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: Evaluation ::: 100%|██████████| 92/92 [00:05<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] recon_loss=0.1687, KL=1406.9236, WMSE=0.0016, MSSSIM=0.1687 | [Val] recon_loss=0.1134, KL=1476.5981, WMSE=0.0010, MSSSIM=0.1134\n",
      "Epoch 2/10 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 2/10: 100%|██████████| 343/343 [00:57<00:00,  6.02it/s, Loss: 0.1168]\n",
      ":: Evaluation ::: 100%|██████████| 92/92 [00:05<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] recon_loss=0.0938, KL=1420.8607, WMSE=0.0009, MSSSIM=0.0938 | [Val] recon_loss=0.0980, KL=1178.9982, WMSE=0.0009, MSSSIM=0.0980\n",
      "Epoch 3/10 - beta_0: 1.0000, beta_1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 3/10:  98%|█████████▊| 335/343 [00:55<00:01,  6.02it/s, Loss: 0.0826]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 117\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - beta_0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobunet_model\u001b[38;5;241m.\u001b[39mbeta_0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobunet_model\u001b[38;5;241m.\u001b[39mbeta_1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# 7a) Train for one epoch (returns mean_crps, mean_kl)\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m train_crps, train_kl, train_wmse, train_msssim \u001b[38;5;241m=\u001b[39m tm\u001b[38;5;241m.\u001b[39mtrain_probunet_step(\n\u001b[1;32m    118\u001b[0m     model\u001b[38;5;241m=\u001b[39mprobunet_model,\n\u001b[1;32m    119\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mdataloader_train,\n\u001b[1;32m    120\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m    121\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    122\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_epochs,\n\u001b[1;32m    123\u001b[0m     device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice,       \n\u001b[1;32m    124\u001b[0m     ensemble_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m    \u001b[38;5;66;03m# how many samples per forward pass\u001b[39;00m\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m train_crps_list\u001b[38;5;241m.\u001b[39mappend(train_crps)\n\u001b[1;32m    127\u001b[0m train_kl_list\u001b[38;5;241m.\u001b[39mappend(train_kl)\n",
      "File \u001b[0;32m~/prob-unet-mds/train_prob_unet_model.py:206\u001b[0m, in \u001b[0;36mtrain_probunet_step\u001b[0;34m(model, dataloader, optimizer, epoch, num_epochs, device, ensemble_size)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# crps_list is just [crps_value], so record it\u001b[39;00m\n\u001b[1;32m    205\u001b[0m recon_vals\u001b[38;5;241m.\u001b[39mappend(recon_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 206\u001b[0m kl_vals\u001b[38;5;241m.\u001b[39mappend(kl_div\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    207\u001b[0m wmse_vals\u001b[38;5;241m.\u001b[39mappend(wmse)\n\u001b[1;32m    208\u001b[0m msssim_vals\u001b[38;5;241m.\u001b[39mappend(msssim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training model with wmse-ms-ssim\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed) \n",
    "        np.random.seed(seed)  \n",
    "        torch.manual_seed(seed) \n",
    "        torch.cuda.manual_seed(seed)  \n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "        torch.backends.cudnn.deterministic = True  \n",
    "        torch.backends.cudnn.benchmark = False  \n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # -- 1) Set seed for reproducibility\n",
    "    set_seed(42)  \n",
    "\n",
    "    # -- 2) Importing all required arguments\n",
    "    args = tm.get_args()\n",
    "    args.lowres_scale = 16\n",
    "    args.batch_size = 32\n",
    "    args.num_epochs = 10\n",
    "\n",
    "    # Initialize the Probabilistic UNet model\n",
    "    probunet_model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=32,\n",
    "        num_filters=[32, 64, 128, 256],\n",
    "        model_channels=32,\n",
    "        channel_mult=[1, 2, 4, 8],\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "\n",
    "    # -- 3) Prepare datasets\n",
    "    dataset_train = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_train,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_val = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_val,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "    dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "    # -- 4) Build DataLoaders\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    dataloader_test_random = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # -- 5) Define optimizer\n",
    "    optimizer = args.optimizer(params=probunet_model.parameters(), lr=args.lr)\n",
    "    # Example alternative:\n",
    "    # optimizer = torch.optim.Adam(probunet_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "    # -- 6) We track CRPS, KL each epoch for train/val\n",
    "    train_crps_list, train_kl_list = [], []\n",
    "    val_crps_list,   val_kl_list = [], []\n",
    "\n",
    "    # For convenience, we keep your adaptive betas:\n",
    "    beta_0 = 1.0\n",
    "    beta_1 = 0.0\n",
    "    warmup_epochs = 2\n",
    "\n",
    "    print(f\"Probabilistic Unet Latent dim: {probunet_model.latent_dim}\")\n",
    "\n",
    "    # -- 7) Main training loop\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        # Set model betas\n",
    "        probunet_model.beta_0 = beta_0\n",
    "        probunet_model.beta_1 = beta_1\n",
    "\n",
    "        print(f\"Epoch {epoch}/{args.num_epochs} - beta_0: {probunet_model.beta_0:.4f}, \"\n",
    "              f\"beta_1: {probunet_model.beta_1:.4f}\")\n",
    "\n",
    "        # 7a) Train for one epoch (returns mean_crps, mean_kl)\n",
    "        train_crps, train_kl, train_wmse, train_msssim = tm.train_probunet_step(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_train,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            num_epochs=args.num_epochs,\n",
    "            device=args.device,       \n",
    "            ensemble_size=1    # how many samples per forward pass\n",
    "        )\n",
    "        train_crps_list.append(train_crps)\n",
    "        train_kl_list.append(train_kl)\n",
    "\n",
    "        # 7b) Update betas after warmup\n",
    "        if epoch > warmup_epochs:\n",
    "            beta_0 = 1.0 / (train_crps + 1e-7)\n",
    "            # beta_0 = 1.0\n",
    "            beta_1 = 1.0 / (train_kl   + 1e-7)\n",
    "        else:\n",
    "            beta_0, beta_1 = 1.0, 0.0\n",
    "\n",
    "        # 7c) Evaluate on validation data\n",
    "        val_crps, val_kl, val_wmse, val_msssim = tm.eval_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_val,\n",
    "            device=args.device,\n",
    "            ensemble_size=1\n",
    "        )\n",
    "        val_crps_list.append(val_crps)\n",
    "        val_kl_list.append(val_kl)\n",
    "\n",
    "        print(f\"[Train] recon_loss={train_crps:.4f}, KL={train_kl:.4f}, WMSE={train_wmse:.4f}, MSSSIM={train_msssim:.4f} | \"\n",
    "              f\"[Val] recon_loss={val_crps:.4f}, KL={val_kl:.4f}, WMSE={val_wmse:.4f}, MSSSIM={val_msssim:.4f}\")\n",
    "        # 7d) Example sampling from the model for sanity checks\n",
    "        test_batch = next(iter(dataloader_test_random))\n",
    "\n",
    "        # Residual predictions\n",
    "        residual_preds, (fig, axs) = tm.sample_residual_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_residuals.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig_difs, axs_difs = dataset_test.plot_residual_differences(\n",
    "            residual_preds=residual_preds,\n",
    "            timestamps_float=test_batch['timestamps_float'][:2],\n",
    "            epoch=epoch,\n",
    "            N=2, \n",
    "            num_samples=3\n",
    "        )\n",
    "        fig_difs.savefig(f\"{args.plotdir}/epoch{epoch}_res_difs.png\", dpi=300)\n",
    "        plt.close(fig_difs)\n",
    "\n",
    "        # Full reconstruction\n",
    "        samples, (fig, axs) = tm.sample_probunet_model(\n",
    "            model=probunet_model,\n",
    "            dataloader=dataloader_test_random,\n",
    "            epoch=epoch,\n",
    "            device=args.device,\n",
    "            batch=test_batch\n",
    "        )\n",
    "        fig.savefig(f\"{args.plotdir}/epoch{epoch}_reconstructed.png\", dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # -- 8) Save final model weights\n",
    "    torch.save(probunet_model.state_dict(),\n",
    "               f\"{args.plotdir}/probunet_model_lat_dim_{probunet_model.latent_dim}.pth\")\n",
    "\n",
    "    # -- 9) Save losses for analysis\n",
    "    losses_to_save = {\n",
    "        \"train_crps\": train_crps_list,\n",
    "        \"train_kl\":   train_kl_list,\n",
    "        \"val_crps\":   val_crps_list,\n",
    "        \"val_kl\":     val_kl_list,\n",
    "    }\n",
    "    with open(f\"{args.plotdir}/losses.pkl\", \"wb\") as f:\n",
    "        pickle.dump(losses_to_save, f)\n",
    "\n",
    "    # -- NEW: Analyze residual contribution\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYZING RESIDUAL CONTRIBUTION\")\n",
    "    print(\"=\"*60)\n",
    "    tm.analyze_residual_contribution(\n",
    "        model=probunet_model,\n",
    "        dataloader=dataloader_test,\n",
    "        device=args.device,\n",
    "        num_samples=20\n",
    "    )\n",
    "  \n",
    "    epochs = np.arange(1, args.num_epochs+1)\n",
    "    plt.plot(epochs, train_crps_list, label='Train CRPS')\n",
    "    plt.plot(epochs, val_crps_list,   label='Val CRPS', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('CRPS')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation CRPS')\n",
    "    plt.savefig(f\"{args.plotdir}/CRPS_curve.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import climex_utils as cu\n",
    "import train_prob_unet_model as tm  \n",
    "from prob_unet import ProbabilisticUNet\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_path, args, latent_dim, num_filters, model_channels, channel_mult):\n",
    "    \"\"\"\n",
    "    Load a trained ProbabilisticUNet model with specified latent_dim, num_filters, model_channels, and channel_mult.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model weights.\n",
    "        args: Arguments for loading.\n",
    "        latent_dim: Latent space dimension for the ProbabilisticUNet.\n",
    "        num_filters: List specifying the number of filters at each U-Net level.\n",
    "        model_channels: Base number of feature maps in the U-Net.\n",
    "        channel_mult: Multipliers for feature maps at each resolution level in the U-Net.\n",
    "\n",
    "    Returns:\n",
    "        Loaded model in evaluation mode.\n",
    "    \"\"\"\n",
    "    model = ProbabilisticUNet(\n",
    "        input_channels=len(args.variables),\n",
    "        num_classes=len(args.variables),\n",
    "        latent_dim=latent_dim,\n",
    "        num_filters=num_filters,\n",
    "        model_channels=model_channels,\n",
    "        channel_mult=channel_mult,\n",
    "        beta_0=0.0,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0  \n",
    "    ).to(args.device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, dataloader_test, num_samples=20, save_dir=\"./test_predictions\"):\n",
    "    \"\"\"\n",
    "    Generate predictions for the entire test set and save the samples to .npy files.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ProbabilisticUNet model.\n",
    "        dataloader_test: DataLoader for the test set.\n",
    "        num_samples: Number of samples per test sample.\n",
    "        save_dir: Directory where .npy files will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    all_preds = []  # Store predictions for all test samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader_test, desc=\"Generating predictions\")):\n",
    "            inputs = batch['inputs'].to(args.device)  \n",
    "            lrinterp = batch['lrinterp'].to(args.device) \n",
    "            timestamps = batch['timestamps'].unsqueeze(dim=1).to(args.device)\n",
    "            hr_targets = batch['hr']  \n",
    "            \n",
    "            sample_preds = []  # Store 50 predictions for this batch\n",
    "            for _ in range(num_samples):\n",
    "                preds = model(inputs, t=timestamps, training=False)  \n",
    "                full_recon = dataloader_test.dataset.residual_to_hr(preds.cpu(), lrinterp.cpu())  \n",
    "                sample_preds.append(full_recon.numpy())  # Convert to NumPy array\n",
    "\n",
    "            # Stack predictions along a new axis (shape: [batch_size, 50, C, H, W])\n",
    "            sample_preds = np.stack(sample_preds, axis=1)\n",
    "            all_preds.append(sample_preds)\n",
    "\n",
    "    # Concatenate all batches (shape: [N, 50, C, H, W])\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    save_path = os.path.join(save_dir, \"predictions.npy\")\n",
    "    np.save(save_path, all_preds)  # Save to .npy file\n",
    "    print(f\"Predictions saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and lazy loading netCDF files\n",
      "Loading dataset into memory\n",
      "Converting xarray Dataset to Pytorch tensor\n",
      "\n",
      "##########################################\n",
      "############ PROCESSING DONE #############\n",
      "##########################################\n",
      "\n",
      "Testing model from ./results/plots/01/16/202511:40:11/probunet_model_lat_dim_16.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|                                                                                                                                      | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:24<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./test_predictions/16_16_Region_64_by_64/predictions.npy\n"
     ]
    }
   ],
   "source": [
    "args = tm.get_args()\n",
    "\n",
    "dataset_test = cu.climex2torch(\n",
    "        datadir=args.datadir,\n",
    "        years=args.years_test,\n",
    "        variables=args.variables,\n",
    "        coords=args.coords,\n",
    "        lowres_scale=args.lowres_scale,\n",
    "        type=\"lrinterp_to_residuals\",\n",
    "        transfo=True\n",
    "    )\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "\n",
    "model_configs = [\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/05/202521:51:55/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/05/202522:14:38/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/06/202500:40:45/probunet_model_lat_dim_8.pth\",\n",
    "    #     \"latent_dim\": 8,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/06/202501:00:11/probunet_model_lat_dim_16.pth\",\n",
    "    #     \"latent_dim\": 16,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202518:33:16/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202521:03:10/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [32, 64, 128, 256],\n",
    "    #     \"model_channels\": 32,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202521:29:20/probunet_model_lat_dim_8.pth\",\n",
    "    #     \"latent_dim\": 8,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202522:01:12/probunet_model_lat_dim_16.pth\",\n",
    "    #     \"latent_dim\": 16,\n",
    "    #     \"num_filters\": [16, 64, 128, 256],\n",
    "    #     \"model_channels\": 16,\n",
    "    #     \"channel_mult\": [1, 4, 8, 16]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/07/202522:21:41/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/08/202512:38:04/probunet_model_lat_dim_32.pth\",\n",
    "    #     \"latent_dim\": 32,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model_path\": \"./results/plots/01/08/202513:41:58/probunet_model_lat_dim_64.pth\",\n",
    "    #     \"latent_dim\": 64,\n",
    "    #     \"num_filters\": [64, 128, 256, 512],\n",
    "    #     \"model_channels\": 64,\n",
    "    #     \"channel_mult\": [1, 2, 4, 8]\n",
    "    # },\n",
    "    {\n",
    "        \"model_path\": \"./results/plots/01/16/202511:40:11/probunet_model_lat_dim_16.pth\",\n",
    "        \"latent_dim\": 16,\n",
    "        \"num_filters\": [16, 64, 128, 256],\n",
    "        \"model_channels\": 16,\n",
    "        \"channel_mult\": [1, 4, 8, 16]\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "for config in model_configs:\n",
    "    print(f\"Testing model from {config['model_path']}\")\n",
    "    \n",
    "    # Load the model\n",
    "    probunet_model = load_model(\n",
    "        model_path=config[\"model_path\"],\n",
    "        args=args,\n",
    "        latent_dim=config[\"latent_dim\"],\n",
    "        num_filters=config[\"num_filters\"],\n",
    "        model_channels=config[\"model_channels\"],\n",
    "        channel_mult=config[\"channel_mult\"]\n",
    "    )\n",
    "\n",
    "    # Generate predictions and save them\n",
    "    generate_samples(probunet_model, dataloader_test, num_samples=20, save_dir=f\"./test_predictions/{config['latent_dim']}_{config['model_channels']}_Region_64_by_64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze residual contribution after training\n",
    "import train_prob_unet_model as tm\n",
    "\n",
    "# After training is complete, analyze how much the model actually helps\n",
    "tm.analyze_residual_contribution(\n",
    "    model=probunet_model,\n",
    "    dataloader=dataloader_test,\n",
    "    device=args.device,\n",
    "    num_samples=20  # Test on 20 samples\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
